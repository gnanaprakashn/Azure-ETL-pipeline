
# ğŸš€ Azure ETL Pipeline (SSMS â†’ ADF â†’ Blob Storage â†’ Databricks â†’ SSMS)

This project demonstrates a complete end-to-end **Azure ETL pipeline** where denormalized data stored in **SQL Server (SSMS)** is normalized using **Azure Data Factory (ADF)** and **Azure Databricks**, transformed into a **star schema**, and loaded back into SQL Server.

This is a real-world enterprise ETL pattern used by data engineering teams.

---

## ğŸ—ï¸ Architecture


<img src="Azure ETL pipeline.png" width="850"/>

**Flow:**  
SSMS â†’ ADF Copy Activity â†’ Blob Storage (Raw) â†’ Databricks Transform â†’ Blob Storage (Processed) â†’ ADF Copy Activity â†’ SSMS (Star Schema)

---

## ğŸ“ Repository Structure

```

Azure-ETL-Pipeline/
â”‚
â”œâ”€â”€ Sample - Superstore_Orders.csv     # Raw denormalized dataset
â”œâ”€â”€ databricks code.py.txt             # PySpark transformation script
â”œâ”€â”€ procedure.txt                      # Step-by-step execution instructions
â”œâ”€â”€ Azure ETL pipeline.png             # Architecture diagram
â””â”€â”€ README.md                          # Project documentation

```

---

## ğŸ”¥ Project Overview

The goal of this project is to:

1. Extract **denormalized** data from SQL Server (SSMS)
2. Load the data into **Azure Blob Storage** using ADF
3. Transform the data using **Databricks PySpark**  
   - Clean  
   - Normalize  
   - Create **dimension** and **fact** tables (Star Schema)
4. Load the cleaned and modeled data back into SSMS for analytics

This automates ETL using Azureâ€™s modern data engineering tools.

---

## ğŸ§© Why This Pipeline?

The source table in SSMS was **heavily denormalized**, containing repeated fields, multiple unrelated attributes, and no proper relational structure.

âœ” We normalized the data  
âœ” Split it into Dimensional & Fact tables  
âœ” Applied business logic  
âœ” Created a clean Star Schema  
âœ” Automated the entire workflow using ADF

This is exactly how Data Engineers transform source OLTP systems into analytic-ready OLAP models.

---

## âš™ï¸ Step-by-Step Pipeline Breakdown

---

### **1ï¸âƒ£ Source Data â€“ SSMS (Denormalized)**

The raw source table contains combined attributes like:
- Customer details  
- Product details  
- Sales metrics  
- City, state, region  
- Order info  

This table must be normalized for BI usage.

---

### **2ï¸âƒ£ Azure Data Factory â€“ Linked Services**

Created Linked Services for:
- **Azure SQL Database (SSMS)**
- **Azure Blob Storage**
- **Azure Databricks**

These secure the connection between Azure services.

---

### **3ï¸âƒ£ ADF â€“ Copy Activity (SSMS â†’ Blob Storage)**

Extracted data from SSMS and stored it in Blob:

```

/raw/superstore_orders.csv

```

ADF ensures reliable extraction with monitoring & retries.

---

### **4ï¸âƒ£ Databricks â€“ Transformation (Normalization + Star Schema)**

PySpark code performs:

âœ” Cleaning (trim, null handling, data types)  
âœ” Removing duplicates  
âœ” Creating **Dimension tables**:
- `dim_customer`
- `dim_product`
- `dim_location`

âœ” Creating **Fact table**:
- `fact_sales`

âœ” Writing transformed tables to:

```

/processed/dim_customer/
/processed/dim_product/
/processed/dim_location/
/processed/fact_sales/

````

> Code used is inside `databricks code.py.txt`

---

### **5ï¸âƒ£ ADF â€“ Load Transformed Data Back to SSMS**

Another Copy Activity loads the processed Parquet files back to:
- `dbo.dim_customer`
- `dbo.dim_product`
- `dbo.dim_location`
- `dbo.fact_sales`

This completes the ETL cycle:
Blob â†’ SSMS

---

## ğŸ“¦ Sample Databricks Code (Summary)

```python
df_raw = spark.read.csv(raw_path, header=True, inferSchema=True)

dim_customer = df_raw.select("Customer Name","Customer Segment") \
                     .dropDuplicates() \
                     .withColumn("customer_id", monotonically_increasing_id())

fact_sales = df_raw.join(dim_customer, "Customer Name") \
                   .select("customer_id","Order Date","Sales")
````

Full code is inside `databricks code.py.txt`.

---

## ğŸ“Š Final Output â€“ Star Schema

### âœ” Dimension Tables

* dim_customer
* dim_product
* dim_location

### âœ” Fact Table

* fact_sales

This schema is BI-ready for:

* Power BI
* Tableau
* Azure Synapse
* Snowflake migration

---

## ğŸ§ª Testing & Validation

âœ” Confirm raw data is extracted to Blob
âœ” Validate Databricks transformations
âœ” Confirm processed Parquet files in Blob
âœ” Test SSMS tables after loading
âœ” End-to-end pipeline run in ADF

---

## ğŸ¯ Skills Demonstrated

* Azure Data Factory (Linked Services, Datasets, Pipelines)
* Azure Blob Storage (raw & processed layers)
* Databricks PySpark transformations
* Star Schema modeling (dim & fact tables)
* ADF copy activity (extraction + loading)
* SSMS SQL Server integration
* Enterprise-grade ETL orchestration

This is a complete Azure Data Engineering workflow.

---

## ğŸ“„ Resume Bullet Points

* Built an end-to-end ETL pipeline using **Azure Data Factory, Blob Storage, Databricks, and SQL Server** to transform denormalized data into a fully normalized star schema.
* Developed PySpark notebooks to clean, normalize, and model the data into **dimension and fact tables**.
* Orchestrated automated data movement between Azure SQL â†’ Blob â†’ Databricks â†’ SQL using ADF pipelines with Linked Services and Copy Activities.

---

## ğŸ‘¤ Author

**Gnana Prakash N**
Data Engineer
GitHub: [gnanaprakashn](https://github.com/gnanaprakashn)

---

## ğŸ“œ License

MIT Â© 2025


